{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ale_py\n",
    "# if using gymnasium\n",
    "import shimmy\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "from torchviz import make_dot, make_dot_from_trace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initilize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.training = False\n",
    "        self.convStack = torch.nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,64, kernel_size=2, stride=1, padding=0),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=3),\n",
    "            nn.ReLU(), # 128x7x7\n",
    "            )\n",
    "        \n",
    "        self.actor_head = torch.nn.Sequential(\n",
    "            nn.Linear(2304, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 5),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "        self.critic_head = torch.nn.Sequential(\n",
    "            nn.Linear(2304, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 1),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.convStack(x)\n",
    "        if self.training:\n",
    "            x = torch.flatten(x, start_dim = 1)\n",
    "        if not self.training:\n",
    "            x = torch.flatten(x, start_dim = 0)\n",
    "        return self.actor_head(x), self.critic_head(x)\n",
    "    \n",
    "    \n",
    "    def calculate_loss(self, G, V, A ):\n",
    "        advantage = G - V\n",
    "        vector = -torch.mul(A.unsqueeze(dim=1),advantage)\n",
    "        loss_actor = vector.sum()\n",
    "        loss_critic = F.smooth_l1_loss(V, G, reduction='sum')\n",
    "        return loss_actor, loss_critic\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    \"\"\"\n",
    "    A class representing an Actor-Critic model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma):\n",
    "        \"\"\"\n",
    "        Initializes the ActorCritic object.\n",
    "\n",
    "        Args:\n",
    "            gamma (float): The discount factor for calculating expected returns.\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.values = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        print(f\"Using {self.device} device\")\n",
    "        self.nn_model = NeuralNetwork().to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.nn_model.parameters(), lr=0.001)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state.\n",
    "\n",
    "        Args:\n",
    "            state: The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the selected action, its log probability, and the predicted value for the state.\n",
    "        \"\"\"\n",
    "        state = torch.tensor(state).to(self.device)\n",
    "        action_probs, value = self.nn_model(state)\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        action_prob = m.log_prob(action)\n",
    "        return action.item(), action_prob, value\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"\n",
    "        Performs a single training step on the neural network.\n",
    "        \"\"\"\n",
    "        V = torch.stack(self.values).to(self.device)\n",
    "        A = torch.stack(self.actions).to(self.device)\n",
    "        G = self.expected_return().to(self.device)\n",
    "        self.optimizer.zero_grad()\n",
    "        actor_loss, critic_loss = self.nn_model.calculate_loss(G, V, A)\n",
    "        loss = actor_loss + critic_loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def save_data(self, action_prob, value, reward):\n",
    "        \"\"\"\n",
    "        Saves the data from a single step in the training process.\n",
    "\n",
    "        Args:\n",
    "            action_prob: The log probability of the selected action.\n",
    "            value: The predicted value for the state.\n",
    "            reward: The reward received for the action.\n",
    "        \"\"\"\n",
    "        self.actions.append(action_prob)\n",
    "        self.values.append(value)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the values, actions, and rewards lists.\n",
    "        \"\"\"\n",
    "        self.values = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def expected_return(self):\n",
    "        \"\"\"\n",
    "        Calculates the expected return (discounted rewards) for the current episode.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The expected return values, normalized by subtracting the mean and dividing by the standard deviation.\n",
    "        \"\"\"\n",
    "        dis_reward = 0\n",
    "        g_array = []\n",
    "        for reward in reversed(self.rewards):\n",
    "            dis_reward = reward + self.gamma * dis_reward\n",
    "            g_array.insert(0, dis_reward)\n",
    "        g = torch.tensor(g_array).unsqueeze(dim=1)\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        g = (g - g.mean()) / (g.std()+eps)\n",
    "        return g "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Episode 0 ended. Episode reward: 9.0 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[39mif\u001b[39;00m step \u001b[39m==\u001b[39m max_steps \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     29\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mMax steps reached.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m     model\u001b[39m.\u001b[39;49mtrain_step()\n\u001b[1;32m     31\u001b[0m     model\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     32\u001b[0m env\u001b[39m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[21], line 56\u001b[0m, in \u001b[0;36mActorCritic.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m loss \u001b[39m=\u001b[39m actor_loss \u001b[39m+\u001b[39m critic_loss\n\u001b[1;32m     55\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 56\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[21], line 56\u001b[0m, in \u001b[0;36mActorCritic.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m loss \u001b[39m=\u001b[39m actor_loss \u001b[39m+\u001b[39m critic_loss\n\u001b[1;32m     55\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 56\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/pyenv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/pyenv/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"ALE/Pacman-v5\",render_mode=\"rgb_array\")\n",
    "#preprocess environment\n",
    "env = gym.wrappers.AtariPreprocessing(env, screen_size=84, grayscale_obs=True, frame_skip=1, noop_max=30, scale_obs = True)\n",
    "torch.manual_seed(543)\n",
    "max_episodes = 10000\n",
    "max_steps = 9999\n",
    "model = ActorCritic(gamma=0.99)\n",
    "for episode in range(max_episodes): \n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    states = []\n",
    "    for step in range(1, max_steps):\n",
    "        if step <= 4:\n",
    "            action = env.action_space.sample()\n",
    "            state, reward, done, _, _ = env.step(action) \n",
    "            states.append(state)\n",
    "        else:\n",
    "            action, action_prob, value = model.act(states)\n",
    "            state, reward, done, _, _ = env.step(action) \n",
    "            states.pop(0)\n",
    "            states.append(state)\n",
    "            model.save_data(action_prob,value,reward)\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            observation, info = env.reset()\n",
    "            print(f\"Episode {episode} ended. Episode reward: {episode_reward} \")\n",
    "            break\n",
    "        if step == max_steps - 1:\n",
    "            print(\"Max steps reached.\")\n",
    "    model.train_step()\n",
    "    model.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.nn_model.state_dict(), \"ac_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
