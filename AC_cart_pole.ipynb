{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir logs\n",
    "! mkdir videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ale_py\n",
    "#import shimmy\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initilize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.common = nn.Linear(4, 128)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.critic = nn.Linear(128,1)\n",
    "        self.actor = nn.Linear(128,2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x)\n",
    "        x = torch.unsqueeze(x,dim=0)\n",
    "        x = self.common(x)\n",
    "        x = self.activation(x)\n",
    "        return self.softmax(self.actor(x)), self.critic(x)\n",
    "    \n",
    "    def calculate_loss(self, G, V, A ):\n",
    "        advantage = G - V\n",
    "        vector = -torch.mul(A,advantage)\n",
    "        loss_actor = vector.sum()\n",
    "        loss_critic = F.smooth_l1_loss(V, G, reduction='sum')\n",
    "        return loss_actor, loss_critic\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    \"\"\"\n",
    "    A class representing an Actor-Critic model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma):\n",
    "        \"\"\"\n",
    "        Initializes the ActorCritic object.\n",
    "\n",
    "        Args:\n",
    "            gamma (float): The discount factor for calculating expected returns.\n",
    "        \"\"\"\n",
    "        self.nn_model = NeuralNetwork()\n",
    "        self.optimizer = torch.optim.Adam(self.nn_model.parameters(), lr=0.005)\n",
    "        self.gamma = gamma\n",
    "        self.values = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state.\n",
    "\n",
    "        Args:\n",
    "            state: The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the selected action, its log probability, and the predicted value for the state.\n",
    "        \"\"\"\n",
    "        action_probs, value = self.nn_model(state)\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        action_prob = m.log_prob(action)\n",
    "        return action.item(), action_prob, value.squeeze(dim=1)\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"\n",
    "        Performs a single training step on the neural network.\n",
    "        \"\"\"\n",
    "        V = torch.stack(self.values)\n",
    "        A = torch.stack(self.actions)\n",
    "        G = self.expected_return()\n",
    "        self.optimizer.zero_grad()\n",
    "        actor_loss, critic_loss = self.nn_model.calculate_loss(G, V, A)\n",
    "        loss = actor_loss + critic_loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def save_data(self, action_prob, value, reward):\n",
    "        \"\"\"\n",
    "        Saves the data from a single step in the training process.\n",
    "\n",
    "        Args:\n",
    "            action_prob: The log probability of the selected action.\n",
    "            value: The predicted value for the state.\n",
    "            reward: The reward received for the action.\n",
    "        \"\"\"\n",
    "        self.actions.append(action_prob)\n",
    "        self.values.append(value)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the values, actions, and rewards lists.\n",
    "        \"\"\"\n",
    "        self.values = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def expected_return(self):\n",
    "        \"\"\"\n",
    "        Calculates the expected return (discounted rewards) for the current episode.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The expected return values, normalized by subtracting the mean and dividing by the standard deviation.\n",
    "        \"\"\"\n",
    "        dis_reward = 0\n",
    "        g_array = []\n",
    "        for reward in reversed(self.rewards):\n",
    "            dis_reward = reward + self.gamma * dis_reward\n",
    "            g_array.insert(0, dis_reward)\n",
    "        g = torch.tensor(g_array).unsqueeze(dim=1)\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        g = (g - g.mean()) / (g.std()+eps)\n",
    "        return g "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konrad/Repos/NCML_project/rlenv/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:87: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/konrad/Repos/NCML_project/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 ended. Episode reward: 15.0 \n",
      "Episode 1 ended. Episode reward: 67.0 \n",
      "Episode 2 ended. Episode reward: 31.0 \n",
      "Episode 3 ended. Episode reward: 11.0 \n",
      "Episode 4 ended. Episode reward: 15.0 \n",
      "Episode 5 ended. Episode reward: 21.0 \n",
      "Episode 6 ended. Episode reward: 24.0 \n",
      "Episode 7 ended. Episode reward: 31.0 \n",
      "Episode 8 ended. Episode reward: 30.0 \n",
      "Episode 9 ended. Episode reward: 11.0 \n",
      "Episode 10 ended. Episode reward: 27.0 \n",
      "Episode 11 ended. Episode reward: 31.0 \n",
      "Episode 12 ended. Episode reward: 11.0 \n",
      "Episode 13 ended. Episode reward: 17.0 \n",
      "Episode 14 ended. Episode reward: 11.0 \n",
      "Episode 15 ended. Episode reward: 15.0 \n",
      "Episode 16 ended. Episode reward: 21.0 \n",
      "Episode 17 ended. Episode reward: 25.0 \n",
      "Episode 18 ended. Episode reward: 16.0 \n",
      "Episode 19 ended. Episode reward: 12.0 \n",
      "Episode 20 ended. Episode reward: 11.0 \n",
      "Episode 21 ended. Episode reward: 40.0 \n",
      "Episode 22 ended. Episode reward: 34.0 \n",
      "Episode 23 ended. Episode reward: 25.0 \n",
      "Episode 24 ended. Episode reward: 25.0 \n",
      "Episode 25 ended. Episode reward: 16.0 \n",
      "Episode 26 ended. Episode reward: 33.0 \n",
      "Episode 27 ended. Episode reward: 24.0 \n",
      "Episode 28 ended. Episode reward: 20.0 \n",
      "Episode 29 ended. Episode reward: 33.0 \n",
      "Episode 30 ended. Episode reward: 23.0 \n",
      "Episode 31 ended. Episode reward: 36.0 \n",
      "Episode 32 ended. Episode reward: 16.0 \n",
      "Episode 33 ended. Episode reward: 17.0 \n",
      "Episode 34 ended. Episode reward: 30.0 \n",
      "Episode 35 ended. Episode reward: 15.0 \n",
      "Episode 36 ended. Episode reward: 14.0 \n",
      "Episode 37 ended. Episode reward: 12.0 \n",
      "Episode 38 ended. Episode reward: 17.0 \n",
      "Episode 39 ended. Episode reward: 41.0 \n",
      "Episode 40 ended. Episode reward: 10.0 \n",
      "Episode 41 ended. Episode reward: 23.0 \n",
      "Episode 42 ended. Episode reward: 15.0 \n",
      "Episode 43 ended. Episode reward: 13.0 \n",
      "Episode 44 ended. Episode reward: 14.0 \n",
      "Episode 45 ended. Episode reward: 23.0 \n",
      "Episode 46 ended. Episode reward: 10.0 \n",
      "Episode 47 ended. Episode reward: 13.0 \n",
      "Episode 48 ended. Episode reward: 11.0 \n",
      "Episode 49 ended. Episode reward: 26.0 \n",
      "Episode 50 ended. Episode reward: 14.0 \n",
      "Episode 51 ended. Episode reward: 9.0 \n",
      "Episode 52 ended. Episode reward: 14.0 \n",
      "Episode 53 ended. Episode reward: 8.0 \n",
      "Episode 54 ended. Episode reward: 16.0 \n",
      "Episode 55 ended. Episode reward: 22.0 \n",
      "Episode 56 ended. Episode reward: 13.0 \n",
      "Episode 57 ended. Episode reward: 9.0 \n",
      "Episode 58 ended. Episode reward: 24.0 \n",
      "Episode 59 ended. Episode reward: 20.0 \n",
      "Episode 60 ended. Episode reward: 20.0 \n",
      "Episode 61 ended. Episode reward: 25.0 \n",
      "Episode 62 ended. Episode reward: 11.0 \n",
      "Episode 63 ended. Episode reward: 20.0 \n",
      "Episode 64 ended. Episode reward: 13.0 \n",
      "Episode 65 ended. Episode reward: 15.0 \n",
      "Episode 66 ended. Episode reward: 11.0 \n",
      "Episode 67 ended. Episode reward: 11.0 \n",
      "Episode 68 ended. Episode reward: 29.0 \n",
      "Episode 69 ended. Episode reward: 32.0 \n",
      "Episode 70 ended. Episode reward: 13.0 \n",
      "Episode 71 ended. Episode reward: 19.0 \n",
      "Episode 72 ended. Episode reward: 20.0 \n",
      "Episode 73 ended. Episode reward: 18.0 \n",
      "Episode 74 ended. Episode reward: 18.0 \n",
      "Episode 75 ended. Episode reward: 12.0 \n",
      "Episode 76 ended. Episode reward: 19.0 \n",
      "Episode 77 ended. Episode reward: 14.0 \n",
      "Episode 78 ended. Episode reward: 19.0 \n",
      "Episode 79 ended. Episode reward: 13.0 \n",
      "Episode 80 ended. Episode reward: 16.0 \n",
      "Episode 81 ended. Episode reward: 38.0 \n",
      "Episode 82 ended. Episode reward: 15.0 \n",
      "Episode 83 ended. Episode reward: 34.0 \n",
      "Episode 84 ended. Episode reward: 22.0 \n",
      "Episode 85 ended. Episode reward: 16.0 \n",
      "Episode 86 ended. Episode reward: 74.0 \n",
      "Episode 87 ended. Episode reward: 16.0 \n",
      "Episode 88 ended. Episode reward: 27.0 \n",
      "Episode 89 ended. Episode reward: 31.0 \n",
      "Episode 90 ended. Episode reward: 30.0 \n",
      "Episode 91 ended. Episode reward: 16.0 \n",
      "Episode 92 ended. Episode reward: 21.0 \n",
      "Episode 93 ended. Episode reward: 17.0 \n",
      "Episode 94 ended. Episode reward: 12.0 \n",
      "Episode 95 ended. Episode reward: 19.0 \n",
      "Episode 96 ended. Episode reward: 13.0 \n",
      "Episode 97 ended. Episode reward: 9.0 \n",
      "Episode 98 ended. Episode reward: 32.0 \n",
      "Moviepy - Building video /home/konrad/Repos/NCML_project/videos/rl-video-episode-99.mp4.\n",
      "Moviepy - Writing video /home/konrad/Repos/NCML_project/videos/rl-video-episode-99.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/konrad/Repos/NCML_project/videos/rl-video-episode-99.mp4\n",
      "Episode 99 ended. Episode reward: 13.0 \n",
      "Episode 100 ended. Episode reward: 23.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 101 ended. Episode reward: 21.0 \n",
      "Episode 102 ended. Episode reward: 18.0 \n",
      "Episode 103 ended. Episode reward: 19.0 \n",
      "Episode 104 ended. Episode reward: 13.0 \n",
      "Episode 105 ended. Episode reward: 22.0 \n",
      "Episode 106 ended. Episode reward: 13.0 \n",
      "Episode 107 ended. Episode reward: 28.0 \n",
      "Episode 108 ended. Episode reward: 40.0 \n",
      "Episode 109 ended. Episode reward: 22.0 \n",
      "Episode 110 ended. Episode reward: 21.0 \n",
      "Episode 111 ended. Episode reward: 18.0 \n",
      "Episode 112 ended. Episode reward: 32.0 \n",
      "Episode 113 ended. Episode reward: 17.0 \n",
      "Episode 114 ended. Episode reward: 13.0 \n",
      "Episode 115 ended. Episode reward: 21.0 \n",
      "Episode 116 ended. Episode reward: 19.0 \n",
      "Episode 117 ended. Episode reward: 15.0 \n",
      "Episode 118 ended. Episode reward: 60.0 \n",
      "Episode 119 ended. Episode reward: 26.0 \n",
      "Episode 120 ended. Episode reward: 53.0 \n",
      "Episode 121 ended. Episode reward: 38.0 \n",
      "Episode 122 ended. Episode reward: 15.0 \n",
      "Episode 123 ended. Episode reward: 13.0 \n",
      "Episode 124 ended. Episode reward: 22.0 \n",
      "Episode 125 ended. Episode reward: 20.0 \n",
      "Episode 126 ended. Episode reward: 11.0 \n",
      "Episode 127 ended. Episode reward: 19.0 \n",
      "Episode 128 ended. Episode reward: 17.0 \n",
      "Episode 129 ended. Episode reward: 13.0 \n",
      "Episode 130 ended. Episode reward: 26.0 \n",
      "Episode 131 ended. Episode reward: 9.0 \n",
      "Episode 132 ended. Episode reward: 17.0 \n",
      "Episode 133 ended. Episode reward: 25.0 \n",
      "Episode 134 ended. Episode reward: 18.0 \n",
      "Episode 135 ended. Episode reward: 15.0 \n",
      "Episode 136 ended. Episode reward: 14.0 \n",
      "Episode 137 ended. Episode reward: 29.0 \n",
      "Episode 138 ended. Episode reward: 25.0 \n",
      "Episode 139 ended. Episode reward: 58.0 \n",
      "Episode 140 ended. Episode reward: 32.0 \n",
      "Episode 141 ended. Episode reward: 63.0 \n",
      "Episode 142 ended. Episode reward: 18.0 \n",
      "Episode 143 ended. Episode reward: 30.0 \n",
      "Episode 144 ended. Episode reward: 17.0 \n",
      "Episode 145 ended. Episode reward: 22.0 \n",
      "Episode 146 ended. Episode reward: 15.0 \n",
      "Episode 147 ended. Episode reward: 26.0 \n",
      "Episode 148 ended. Episode reward: 11.0 \n",
      "Episode 149 ended. Episode reward: 13.0 \n",
      "Episode 150 ended. Episode reward: 12.0 \n",
      "Episode 151 ended. Episode reward: 13.0 \n",
      "Episode 152 ended. Episode reward: 10.0 \n",
      "Episode 153 ended. Episode reward: 17.0 \n",
      "Episode 154 ended. Episode reward: 13.0 \n",
      "Episode 155 ended. Episode reward: 12.0 \n",
      "Episode 156 ended. Episode reward: 10.0 \n",
      "Episode 157 ended. Episode reward: 24.0 \n",
      "Episode 158 ended. Episode reward: 22.0 \n",
      "Episode 159 ended. Episode reward: 19.0 \n",
      "Episode 160 ended. Episode reward: 15.0 \n",
      "Episode 161 ended. Episode reward: 10.0 \n",
      "Episode 162 ended. Episode reward: 18.0 \n",
      "Episode 163 ended. Episode reward: 20.0 \n",
      "Episode 164 ended. Episode reward: 14.0 \n",
      "Episode 165 ended. Episode reward: 29.0 \n",
      "Episode 166 ended. Episode reward: 15.0 \n",
      "Episode 167 ended. Episode reward: 24.0 \n",
      "Episode 168 ended. Episode reward: 14.0 \n",
      "Episode 169 ended. Episode reward: 16.0 \n",
      "Episode 170 ended. Episode reward: 23.0 \n",
      "Episode 171 ended. Episode reward: 22.0 \n",
      "Episode 172 ended. Episode reward: 16.0 \n",
      "Episode 173 ended. Episode reward: 15.0 \n",
      "Episode 174 ended. Episode reward: 14.0 \n",
      "Episode 175 ended. Episode reward: 16.0 \n",
      "Episode 176 ended. Episode reward: 16.0 \n",
      "Episode 177 ended. Episode reward: 25.0 \n",
      "Episode 178 ended. Episode reward: 16.0 \n",
      "Episode 179 ended. Episode reward: 26.0 \n",
      "Episode 180 ended. Episode reward: 31.0 \n",
      "Episode 181 ended. Episode reward: 29.0 \n",
      "Episode 182 ended. Episode reward: 42.0 \n",
      "Episode 183 ended. Episode reward: 49.0 \n",
      "Episode 184 ended. Episode reward: 96.0 \n",
      "Episode 185 ended. Episode reward: 31.0 \n",
      "Episode 186 ended. Episode reward: 58.0 \n",
      "Episode 187 ended. Episode reward: 65.0 \n",
      "Episode 188 ended. Episode reward: 48.0 \n",
      "Episode 189 ended. Episode reward: 34.0 \n",
      "Episode 190 ended. Episode reward: 45.0 \n",
      "Episode 191 ended. Episode reward: 53.0 \n",
      "Episode 192 ended. Episode reward: 49.0 \n",
      "Episode 193 ended. Episode reward: 27.0 \n",
      "Episode 194 ended. Episode reward: 62.0 \n",
      "Episode 195 ended. Episode reward: 103.0 \n",
      "Episode 196 ended. Episode reward: 32.0 \n",
      "Episode 197 ended. Episode reward: 72.0 \n",
      "Episode 198 ended. Episode reward: 30.0 \n",
      "Moviepy - Building video /home/konrad/Repos/NCML_project/videos/rl-video-episode-199.mp4.\n",
      "Moviepy - Writing video /home/konrad/Repos/NCML_project/videos/rl-video-episode-199.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/konrad/Repos/NCML_project/videos/rl-video-episode-199.mp4\n",
      "Episode 199 ended. Episode reward: 21.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200 ended. Episode reward: 36.0 \n",
      "Episode 201 ended. Episode reward: 31.0 \n",
      "Episode 202 ended. Episode reward: 35.0 \n",
      "Episode 203 ended. Episode reward: 45.0 \n",
      "Episode 204 ended. Episode reward: 81.0 \n",
      "Episode 205 ended. Episode reward: 48.0 \n",
      "Episode 206 ended. Episode reward: 60.0 \n",
      "Episode 207 ended. Episode reward: 39.0 \n",
      "Episode 208 ended. Episode reward: 78.0 \n",
      "Episode 209 ended. Episode reward: 70.0 \n",
      "Episode 210 ended. Episode reward: 55.0 \n",
      "Episode 211 ended. Episode reward: 86.0 \n",
      "Episode 212 ended. Episode reward: 85.0 \n",
      "Episode 213 ended. Episode reward: 68.0 \n",
      "Episode 214 ended. Episode reward: 37.0 \n",
      "Episode 215 ended. Episode reward: 48.0 \n",
      "Episode 216 ended. Episode reward: 124.0 \n",
      "Episode 217 ended. Episode reward: 102.0 \n",
      "Episode 218 ended. Episode reward: 34.0 \n",
      "Episode 219 ended. Episode reward: 41.0 \n",
      "Episode 220 ended. Episode reward: 68.0 \n",
      "Episode 221 ended. Episode reward: 79.0 \n",
      "Episode 222 ended. Episode reward: 54.0 \n",
      "Episode 223 ended. Episode reward: 72.0 \n",
      "Episode 224 ended. Episode reward: 111.0 \n",
      "Episode 225 ended. Episode reward: 39.0 \n",
      "Episode 226 ended. Episode reward: 82.0 \n",
      "Episode 227 ended. Episode reward: 27.0 \n",
      "Episode 228 ended. Episode reward: 51.0 \n",
      "Episode 229 ended. Episode reward: 75.0 \n",
      "Episode 230 ended. Episode reward: 39.0 \n",
      "Episode 231 ended. Episode reward: 76.0 \n",
      "Episode 232 ended. Episode reward: 56.0 \n",
      "Episode 233 ended. Episode reward: 85.0 \n",
      "Episode 234 ended. Episode reward: 92.0 \n",
      "Episode 235 ended. Episode reward: 41.0 \n",
      "Episode 236 ended. Episode reward: 58.0 \n",
      "Episode 237 ended. Episode reward: 65.0 \n",
      "Episode 238 ended. Episode reward: 138.0 \n",
      "Episode 239 ended. Episode reward: 49.0 \n",
      "Episode 240 ended. Episode reward: 52.0 \n",
      "Episode 241 ended. Episode reward: 38.0 \n",
      "Episode 242 ended. Episode reward: 73.0 \n",
      "Episode 243 ended. Episode reward: 23.0 \n",
      "Episode 244 ended. Episode reward: 49.0 \n",
      "Episode 245 ended. Episode reward: 86.0 \n",
      "Episode 246 ended. Episode reward: 70.0 \n",
      "Episode 247 ended. Episode reward: 66.0 \n",
      "Episode 248 ended. Episode reward: 36.0 \n",
      "Episode 249 ended. Episode reward: 26.0 \n",
      "Episode 250 ended. Episode reward: 39.0 \n",
      "Episode 251 ended. Episode reward: 28.0 \n",
      "Episode 252 ended. Episode reward: 42.0 \n",
      "Episode 253 ended. Episode reward: 47.0 \n",
      "Episode 254 ended. Episode reward: 68.0 \n",
      "Episode 255 ended. Episode reward: 73.0 \n",
      "Episode 256 ended. Episode reward: 46.0 \n",
      "Episode 257 ended. Episode reward: 61.0 \n",
      "Episode 258 ended. Episode reward: 25.0 \n",
      "Episode 259 ended. Episode reward: 67.0 \n",
      "Episode 260 ended. Episode reward: 37.0 \n",
      "Episode 261 ended. Episode reward: 23.0 \n",
      "Episode 262 ended. Episode reward: 34.0 \n",
      "Episode 263 ended. Episode reward: 43.0 \n",
      "Episode 264 ended. Episode reward: 41.0 \n",
      "Episode 265 ended. Episode reward: 52.0 \n",
      "Episode 266 ended. Episode reward: 28.0 \n",
      "Episode 267 ended. Episode reward: 63.0 \n",
      "Episode 268 ended. Episode reward: 28.0 \n",
      "Episode 269 ended. Episode reward: 45.0 \n",
      "Episode 270 ended. Episode reward: 39.0 \n",
      "Episode 271 ended. Episode reward: 37.0 \n",
      "Episode 272 ended. Episode reward: 28.0 \n",
      "Episode 273 ended. Episode reward: 46.0 \n",
      "Episode 274 ended. Episode reward: 46.0 \n",
      "Episode 275 ended. Episode reward: 67.0 \n",
      "Episode 276 ended. Episode reward: 94.0 \n",
      "Episode 277 ended. Episode reward: 86.0 \n",
      "Episode 278 ended. Episode reward: 88.0 \n",
      "Episode 279 ended. Episode reward: 68.0 \n",
      "Episode 280 ended. Episode reward: 43.0 \n",
      "Episode 281 ended. Episode reward: 65.0 \n",
      "Episode 282 ended. Episode reward: 43.0 \n",
      "Episode 283 ended. Episode reward: 105.0 \n",
      "Episode 284 ended. Episode reward: 136.0 \n",
      "Episode 285 ended. Episode reward: 88.0 \n",
      "Episode 286 ended. Episode reward: 61.0 \n",
      "Episode 287 ended. Episode reward: 44.0 \n",
      "Episode 288 ended. Episode reward: 107.0 \n",
      "Episode 289 ended. Episode reward: 98.0 \n",
      "Episode 290 ended. Episode reward: 92.0 \n",
      "Episode 291 ended. Episode reward: 93.0 \n",
      "Episode 292 ended. Episode reward: 130.0 \n",
      "Episode 293 ended. Episode reward: 67.0 \n",
      "Episode 294 ended. Episode reward: 93.0 \n",
      "Episode 295 ended. Episode reward: 67.0 \n",
      "Episode 296 ended. Episode reward: 105.0 \n",
      "Episode 297 ended. Episode reward: 46.0 \n",
      "Episode 298 ended. Episode reward: 74.0 \n",
      "Moviepy - Building video /home/konrad/Repos/NCML_project/videos/rl-video-episode-299.mp4.\n",
      "Moviepy - Writing video /home/konrad/Repos/NCML_project/videos/rl-video-episode-299.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/konrad/Repos/NCML_project/videos/rl-video-episode-299.mp4\n",
      "Episode 299 ended. Episode reward: 58.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300 ended. Episode reward: 99.0 \n",
      "Episode 301 ended. Episode reward: 84.0 \n",
      "Episode 302 ended. Episode reward: 85.0 \n",
      "Episode 303 ended. Episode reward: 53.0 \n",
      "Episode 304 ended. Episode reward: 65.0 \n",
      "Episode 305 ended. Episode reward: 79.0 \n",
      "Episode 306 ended. Episode reward: 56.0 \n",
      "Episode 307 ended. Episode reward: 49.0 \n",
      "Episode 308 ended. Episode reward: 63.0 \n",
      "Episode 309 ended. Episode reward: 38.0 \n",
      "Episode 310 ended. Episode reward: 27.0 \n",
      "Episode 311 ended. Episode reward: 35.0 \n",
      "Episode 312 ended. Episode reward: 55.0 \n",
      "Episode 313 ended. Episode reward: 51.0 \n",
      "Episode 314 ended. Episode reward: 61.0 \n",
      "Episode 315 ended. Episode reward: 37.0 \n",
      "Episode 316 ended. Episode reward: 79.0 \n",
      "Episode 317 ended. Episode reward: 74.0 \n",
      "Episode 318 ended. Episode reward: 96.0 \n",
      "Episode 319 ended. Episode reward: 73.0 \n",
      "Episode 320 ended. Episode reward: 166.0 \n",
      "Episode 321 ended. Episode reward: 64.0 \n",
      "Episode 322 ended. Episode reward: 110.0 \n",
      "Episode 323 ended. Episode reward: 120.0 \n",
      "Episode 324 ended. Episode reward: 93.0 \n",
      "Episode 325 ended. Episode reward: 128.0 \n",
      "Episode 326 ended. Episode reward: 152.0 \n",
      "Episode 327 ended. Episode reward: 125.0 \n",
      "Episode 328 ended. Episode reward: 119.0 \n",
      "Episode 329 ended. Episode reward: 209.0 \n",
      "Episode 330 ended. Episode reward: 167.0 \n",
      "Episode 331 ended. Episode reward: 231.0 \n",
      "Episode 332 ended. Episode reward: 94.0 \n",
      "Episode 333 ended. Episode reward: 105.0 \n",
      "Episode 334 ended. Episode reward: 196.0 \n",
      "Episode 335 ended. Episode reward: 157.0 \n",
      "Episode 336 ended. Episode reward: 149.0 \n",
      "Episode 337 ended. Episode reward: 113.0 \n",
      "Episode 338 ended. Episode reward: 94.0 \n",
      "Episode 339 ended. Episode reward: 201.0 \n",
      "Episode 340 ended. Episode reward: 108.0 \n",
      "Episode 341 ended. Episode reward: 119.0 \n",
      "Episode 342 ended. Episode reward: 126.0 \n",
      "Episode 343 ended. Episode reward: 153.0 \n",
      "Episode 344 ended. Episode reward: 166.0 \n",
      "Episode 345 ended. Episode reward: 233.0 \n",
      "Episode 346 ended. Episode reward: 371.0 \n",
      "Episode 347 ended. Episode reward: 202.0 \n",
      "Episode 348 ended. Episode reward: 170.0 \n",
      "Episode 349 ended. Episode reward: 243.0 \n",
      "Episode 350 ended. Episode reward: 153.0 \n",
      "Episode 351 ended. Episode reward: 229.0 \n",
      "Episode 352 ended. Episode reward: 173.0 \n",
      "Episode 353 ended. Episode reward: 162.0 \n",
      "Episode 354 ended. Episode reward: 196.0 \n",
      "Episode 355 ended. Episode reward: 135.0 \n",
      "Episode 356 ended. Episode reward: 141.0 \n",
      "Episode 357 ended. Episode reward: 152.0 \n",
      "Episode 358 ended. Episode reward: 601.0 \n",
      "Episode 359 ended. Episode reward: 292.0 \n",
      "Episode 360 ended. Episode reward: 189.0 \n",
      "Episode 361 ended. Episode reward: 230.0 \n",
      "Episode 362 ended. Episode reward: 330.0 \n",
      "Episode 363 ended. Episode reward: 516.0 \n",
      "Episode 364 ended. Episode reward: 353.0 \n",
      "Episode 365 ended. Episode reward: 400.0 \n",
      "Episode 366 ended. Episode reward: 248.0 \n",
      "Episode 367 ended. Episode reward: 277.0 \n",
      "Episode 368 ended. Episode reward: 262.0 \n",
      "Episode 369 ended. Episode reward: 184.0 \n",
      "Episode 370 ended. Episode reward: 226.0 \n",
      "Episode 371 ended. Episode reward: 228.0 \n",
      "Episode 372 ended. Episode reward: 286.0 \n",
      "Episode 373 ended. Episode reward: 218.0 \n",
      "Episode 374 ended. Episode reward: 142.0 \n",
      "Episode 375 ended. Episode reward: 289.0 \n",
      "Episode 376 ended. Episode reward: 203.0 \n",
      "Episode 377 ended. Episode reward: 101.0 \n",
      "Episode 378 ended. Episode reward: 65.0 \n",
      "Episode 379 ended. Episode reward: 331.0 \n",
      "Episode 380 ended. Episode reward: 260.0 \n",
      "Episode 381 ended. Episode reward: 92.0 \n",
      "Episode 382 ended. Episode reward: 122.0 \n",
      "Episode 383 ended. Episode reward: 182.0 \n",
      "Episode 384 ended. Episode reward: 93.0 \n",
      "Episode 385 ended. Episode reward: 128.0 \n",
      "Episode 386 ended. Episode reward: 125.0 \n",
      "Episode 387 ended. Episode reward: 153.0 \n",
      "Episode 388 ended. Episode reward: 544.0 \n",
      "Episode 389 ended. Episode reward: 238.0 \n",
      "Episode 390 ended. Episode reward: 518.0 \n",
      "Episode 391 ended. Episode reward: 238.0 \n",
      "Episode 392 ended. Episode reward: 545.0 \n",
      "Episode 393 ended. Episode reward: 320.0 \n",
      "Episode 394 ended. Episode reward: 262.0 \n",
      "Episode 395 ended. Episode reward: 271.0 \n",
      "Episode 396 ended. Episode reward: 1195.0 \n",
      "Episode 397 ended. Episode reward: 359.0 \n",
      "Episode 398 ended. Episode reward: 678.0 \n",
      "Moviepy - Building video /home/konrad/Repos/NCML_project/videos/rl-video-episode-399.mp4.\n",
      "Moviepy - Writing video /home/konrad/Repos/NCML_project/videos/rl-video-episode-399.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/konrad/Repos/NCML_project/videos/rl-video-episode-399.mp4\n",
      "Episode 399 ended. Episode reward: 471.0 \n",
      "Episode 400 ended. Episode reward: 302.0 \n",
      "Episode 401 ended. Episode reward: 358.0 \n",
      "Episode 402 ended. Episode reward: 398.0 \n",
      "Episode 403 ended. Episode reward: 357.0 \n",
      "Episode 404 ended. Episode reward: 264.0 \n",
      "Episode 405 ended. Episode reward: 234.0 \n",
      "Episode 406 ended. Episode reward: 293.0 \n",
      "Episode 407 ended. Episode reward: 245.0 \n",
      "Episode 408 ended. Episode reward: 329.0 \n",
      "Episode 409 ended. Episode reward: 306.0 \n",
      "Episode 410 ended. Episode reward: 482.0 \n",
      "Episode 411 ended. Episode reward: 521.0 \n",
      "Episode 412 ended. Episode reward: 837.0 \n",
      "Episode 413 ended. Episode reward: 814.0 \n",
      "Episode 414 ended. Episode reward: 907.0 \n",
      "Episode 415 ended. Episode reward: 1779.0 \n",
      "Episode 416 ended. Episode reward: 1786.0 \n",
      "Episode 417 ended. Episode reward: 1663.0 \n",
      "Episode 418 ended. Episode reward: 991.0 \n",
      "Episode 419 ended. Episode reward: 764.0 \n",
      "Episode 420 ended. Episode reward: 2062.0 \n",
      "Episode 421 ended. Episode reward: 2848.0 \n",
      "Episode 422 ended. Episode reward: 2625.0 \n",
      "Episode 423 ended. Episode reward: 770.0 \n",
      "Episode 424 ended. Episode reward: 1261.0 \n",
      "Episode 425 ended. Episode reward: 3051.0 \n",
      "Episode 426 ended. Episode reward: 1513.0 \n",
      "Episode 427 ended. Episode reward: 4694.0 \n",
      "Episode 428 ended. Episode reward: 8889.0 \n",
      "Max steps reached.\n",
      "Episode 430 ended. Episode reward: 722.0 \n",
      "Episode 431 ended. Episode reward: 1037.0 \n",
      "Episode 432 ended. Episode reward: 576.0 \n",
      "Episode 433 ended. Episode reward: 475.0 \n",
      "Episode 434 ended. Episode reward: 415.0 \n",
      "Episode 435 ended. Episode reward: 705.0 \n",
      "Episode 436 ended. Episode reward: 457.0 \n",
      "Episode 437 ended. Episode reward: 639.0 \n",
      "Episode 438 ended. Episode reward: 1113.0 \n",
      "Episode 439 ended. Episode reward: 837.0 \n",
      "Episode 440 ended. Episode reward: 1457.0 \n",
      "Episode 441 ended. Episode reward: 5940.0 \n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Episode 444 ended. Episode reward: 6882.0 \n",
      "Episode 445 ended. Episode reward: 9857.0 \n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Episode 454 ended. Episode reward: 2042.0 \n",
      "Episode 455 ended. Episode reward: 1178.0 \n",
      "Episode 456 ended. Episode reward: 7858.0 \n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Episode 464 ended. Episode reward: 107.0 \n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Episode 472 ended. Episode reward: 224.0 \n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Max steps reached.\n",
      "Episode 476 ended. Episode reward: 120.0 \n",
      "Episode 477 ended. Episode reward: 119.0 \n",
      "Episode 478 ended. Episode reward: 122.0 \n",
      "Episode 479 ended. Episode reward: 107.0 \n",
      "Episode 480 ended. Episode reward: 115.0 \n",
      "Episode 481 ended. Episode reward: 119.0 \n",
      "Episode 482 ended. Episode reward: 103.0 \n",
      "Episode 483 ended. Episode reward: 33.0 \n",
      "Episode 484 ended. Episode reward: 102.0 \n",
      "Episode 485 ended. Episode reward: 115.0 \n",
      "Episode 486 ended. Episode reward: 44.0 \n",
      "Episode 487 ended. Episode reward: 48.0 \n",
      "Episode 488 ended. Episode reward: 111.0 \n",
      "Episode 489 ended. Episode reward: 105.0 \n",
      "Episode 490 ended. Episode reward: 108.0 \n",
      "Episode 491 ended. Episode reward: 95.0 \n",
      "Episode 492 ended. Episode reward: 112.0 \n",
      "Episode 493 ended. Episode reward: 105.0 \n",
      "Episode 494 ended. Episode reward: 113.0 \n",
      "Episode 495 ended. Episode reward: 41.0 \n",
      "Episode 496 ended. Episode reward: 99.0 \n",
      "Episode 497 ended. Episode reward: 108.0 \n",
      "Episode 498 ended. Episode reward: 101.0 \n",
      "Moviepy - Building video /home/konrad/Repos/NCML_project/videos/rl-video-episode-499.mp4.\n",
      "Moviepy - Writing video /home/konrad/Repos/NCML_project/videos/rl-video-episode-499.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/konrad/Repos/NCML_project/videos/rl-video-episode-499.mp4\n",
      "Episode 499 ended. Episode reward: 96.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\",render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.RecordVideo(env, './videos', episode_trigger = lambda x: (x+1) % 100 == 0)\n",
    "torch.manual_seed(543)\n",
    "max_episodes = 500\n",
    "max_steps = 9999\n",
    "model = ActorCritic(gamma=0.99)\n",
    "episodes = []\n",
    "losses = []\n",
    "logger = {\"episodes\":[],\"losses\":[]}\n",
    "for episode in range(max_episodes): \n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    steps = 0\n",
    "    for step in range(1, max_steps):\n",
    "        action, action_prob, value = model.act(state)\n",
    "        state, reward, done, _, _ = env.step(action) \n",
    "        model.save_data(action_prob,value,reward)\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            observation, info = env.reset()\n",
    "            print(f\"Episode {episode} ended. Episode reward: {episode_reward} \")\n",
    "            break\n",
    "        if step == max_steps - 1:\n",
    "            print(\"Max steps reached.\")\n",
    "        steps = step\n",
    "    episode_loss = model.train_step()\n",
    "    model.reset()\n",
    "    if episode % 10 == 0:\n",
    "            logger[\"episodes\"].append(episodes)\n",
    "            logger[\"losses\"].append(losses)\n",
    "            episodes = []\n",
    "            losses = []\n",
    "    episodes.append(episode_reward)\n",
    "    losses.append(episode_loss.item())\n",
    "env.close()\n",
    "with open('logs/logger_dqn.json', 'w') as fp:\n",
    "        json.dump(logger, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
