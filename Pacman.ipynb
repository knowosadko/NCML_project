{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12eac9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in ./rlenv/lib/python3.10/site-packages (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./rlenv/lib/python3.10/site-packages (from gymnasium) (1.24.3)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in ./rlenv/lib/python3.10/site-packages (from gymnasium) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./rlenv/lib/python3.10/site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./rlenv/lib/python3.10/site-packages (from gymnasium) (4.6.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./rlenv/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: gymnasium[accept-rom-license,atari] in ./rlenv/lib/python3.10/site-packages (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./rlenv/lib/python3.10/site-packages (from gymnasium[accept-rom-license,atari]) (1.24.3)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in ./rlenv/lib/python3.10/site-packages (from gymnasium[accept-rom-license,atari]) (1.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in ./rlenv/lib/python3.10/site-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in ./rlenv/lib/python3.10/site-packages (from gymnasium[accept-rom-license,atari]) (4.6.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in ./rlenv/lib/python3.10/site-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
      "Requirement already satisfied: shimmy[atari]<1.0,>=0.1.0 in ./rlenv/lib/python3.10/site-packages (from gymnasium[accept-rom-license,atari]) (0.2.1)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in ./rlenv/lib/python3.10/site-packages (from gymnasium[accept-rom-license,atari]) (0.4.2)\n",
      "Requirement already satisfied: click in ./rlenv/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (8.1.3)\n",
      "Requirement already satisfied: requests in ./rlenv/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.31.0)\n",
      "Requirement already satisfied: tqdm in ./rlenv/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (4.65.0)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in ./rlenv/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (0.6.1)\n",
      "Requirement already satisfied: ale-py~=0.8.1 in ./rlenv/lib/python3.10/site-packages (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (0.8.1)\n",
      "Requirement already satisfied: importlib-resources in ./rlenv/lib/python3.10/site-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (5.12.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./rlenv/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./rlenv/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./rlenv/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./rlenv/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2023.5.7)\n",
      "Requirement already satisfied: torch in ./rlenv/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in ./rlenv/lib/python3.10/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in ./rlenv/lib/python3.10/site-packages (from torch) (4.6.2)\n",
      "Requirement already satisfied: sympy in ./rlenv/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in ./rlenv/lib/python3.10/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./rlenv/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./rlenv/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./rlenv/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./rlenv/lib/python3.10/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./rlenv/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./rlenv/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./rlenv/lib/python3.10/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./rlenv/lib/python3.10/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./rlenv/lib/python3.10/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./rlenv/lib/python3.10/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./rlenv/lib/python3.10/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./rlenv/lib/python3.10/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./rlenv/lib/python3.10/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: setuptools in ./rlenv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.4.0)\n",
      "Requirement already satisfied: wheel in ./rlenv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: cmake in ./rlenv/lib/python3.10/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
      "Requirement already satisfied: lit in ./rlenv/lib/python3.10/site-packages (from triton==2.0.0->torch) (16.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./rlenv/lib/python3.10/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./rlenv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: torchvision in ./rlenv/lib/python3.10/site-packages (0.15.2)\n",
      "Requirement already satisfied: numpy in ./rlenv/lib/python3.10/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in ./rlenv/lib/python3.10/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.0.1 in ./rlenv/lib/python3.10/site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./rlenv/lib/python3.10/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: filelock in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (4.6.2)\n",
      "Requirement already satisfied: sympy in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in ./rlenv/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
      "Requirement already satisfied: setuptools in ./rlenv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision) (65.4.0)\n",
      "Requirement already satisfied: wheel in ./rlenv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision) (0.37.1)\n",
      "Requirement already satisfied: cmake in ./rlenv/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.26.3)\n",
      "Requirement already satisfied: lit in ./rlenv/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./rlenv/lib/python3.10/site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./rlenv/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./rlenv/lib/python3.10/site-packages (from requests->torchvision) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./rlenv/lib/python3.10/site-packages (from requests->torchvision) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./rlenv/lib/python3.10/site-packages (from jinja2->torch==2.0.1->torchvision) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./rlenv/lib/python3.10/site-packages (from sympy->torch==2.0.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: numpy in ./rlenv/lib/python3.10/site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in ./rlenv/lib/python3.10/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./rlenv/lib/python3.10/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in ./rlenv/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./rlenv/lib/python3.10/site-packages (from matplotlib) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./rlenv/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in ./rlenv/lib/python3.10/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./rlenv/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./rlenv/lib/python3.10/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./rlenv/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./rlenv/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./rlenv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install gymnasium\n",
    "! pip install \"gymnasium[atari, accept-rom-license]\"\n",
    "! pip install torch\n",
    "! pip install torchvision\n",
    "! pip install numpy\n",
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a753e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘videos’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir logs\n",
    "! mkdir videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343f5217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ale_py\n",
    "#import shimmy\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a15bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()   \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=64*7*7 , out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x)\n",
    "        x = torch.flatten(conv_out, start_dim=1)\n",
    "        return self.fc(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5dce59e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    \n",
    "    def __init__(self,size):\n",
    "        self.size = size\n",
    "        self.experiences = []\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        return random.choices(self.experiences, k=batch_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.experiences.append(experience)\n",
    "        if len(self.experiences) > self.size:\n",
    "            self.experiences.pop(0)\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.experiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f008d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent:\n",
    "\n",
    "\n",
    "    def __init__(self, lr=0.0001 ,gamma=0.99, epsilon_params=(0.9,0.05,1000)):\n",
    "        # Get cpu, gpu or mps device for training.\n",
    "        self.device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        print(f\"Using {self.device} device\")\n",
    "        self.pred_NN = CNN().to(self.device)\n",
    "        self.target_NN = copy.deepcopy(self.pred_NN)\n",
    "        self.target_NN.eval()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_params[0]\n",
    "        self.epsilon_end = epsilon_params[1]\n",
    "        self.epsilon_decay = epsilon_params[2]\n",
    "        self.optimizer = torch.optim.RMSprop(self.pred_NN.parameters(), lr=lr)\n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def predict(self, x):\n",
    "        self.steps_done += 1\n",
    "        return self.pred_NN.forward(x)\n",
    "    \n",
    "    def action(self, pred):\n",
    "        eps = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * math.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        return (\n",
    "            random.randint(0, pred.size(dim=0) - 1)\n",
    "            if random.random() < eps\n",
    "            else torch.argmax(pred).item()\n",
    "        )\n",
    "    \n",
    "    def train(self, experience_batch):\n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "        epoch_loss = 0\n",
    "        states = torch.stack([experience_batch[i][0].squeeze(0) for i in range(len(experience_batch))]).to(self.device)\n",
    "        actions = torch.tensor([experience_batch[i][1] for i in range(len(experience_batch))])\n",
    "        rewards = torch.tensor([experience_batch[i][2] for i in range(len(experience_batch))]).to(self.device)#torch.tensor(experience_batch[:][2])\n",
    "        next_states = torch.stack([experience_batch[i][3].squeeze(0)  for i in range(len(experience_batch))]).to(self.device)\n",
    "        terminated = torch.tensor([not experience_batch[i][4] for i in range(len(experience_batch))]).to(self.device)\n",
    "        y = self.estimated_value( rewards, next_states, terminated)\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self.pred_NN(states)\n",
    "        indicies = torch.LongTensor(actions)\n",
    "        indicies =indicies.unsqueeze(dim=0).T\n",
    "        pred = pred.gather(1,indicies.to(self.device))\n",
    "        loss = loss_fn(y, pred)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        return epoch_loss\n",
    "        \n",
    "    def copy(self):\n",
    "        self.target_NN.load_state_dict(self.pred_NN.state_dict())  \n",
    "        \n",
    "    def estimated_value(self, reward, next_state, done):\n",
    "        with torch.no_grad():# vectorize it\n",
    "                target_pred = self.target_NN.forward(next_state.to(self.device))\n",
    "                max_pred = torch.max(target_pred,1)[0].unsqueeze(1)\n",
    "                done = done.unsqueeze(1)\n",
    "                target = reward.unsqueeze(1) + self.gamma * torch.mul(max_pred,done)\n",
    "        return target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b4d1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_to_torch(t):\n",
    "    t = t.unsqueeze(dim=0)\n",
    "    return torch.movedim(t, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7d4bd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konrad/Repos/NCML_project/rlenv/lib/python3.10/site-packages/gymnasium/wrappers/record_video.py:87: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/konrad/Repos/NCML_project/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Episode: 1 Reward: 1.0 loss: 0.0 last rewards: 1.0\n",
      "Episode: 2 Reward: 1.0 loss: 0.0 last rewards: 1.0\n",
      "Episode: 3 Reward: 8.0 loss: 0.0021332684939934224 last rewards: 3.3333333333333335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konrad/Repos/NCML_project/rlenv/lib/python3.10/site-packages/gymnasium/utils/passive_env_checker.py:364: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4 Reward: 2.0 loss: 0.004169668286619468 last rewards: 3.0\n",
      "Moviepy - Building video /home/konrad/Repos/NCML_project/videos/rl-video-episode-4.mp4.\n",
      "Moviepy - Writing video /home/konrad/Repos/NCML_project/videos/rl-video-episode-4.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/konrad/Repos/NCML_project/videos/rl-video-episode-4.mp4\n",
      "Episode: 5 Reward: 3.0 loss: 0.0035990033330712485 last rewards: 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 6 Reward: 3.0 loss: 0.0031959493024910986 last rewards: 3.0\n",
      "Episode: 7 Reward: 8.0 loss: 0.002635872571649458 last rewards: 3.7142857142857144\n",
      "Episode: 8 Reward: 4.0 loss: 0.0030655450593332634 last rewards: 3.75\n",
      "Episode: 9 Reward: 12.0 loss: 0.002775955599896579 last rewards: 4.666666666666667\n",
      "Moviepy - Building video /home/konrad/Repos/NCML_project/videos/rl-video-episode-9.mp4.\n",
      "Moviepy - Writing video /home/konrad/Repos/NCML_project/videos/rl-video-episode-9.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/konrad/Repos/NCML_project/videos/rl-video-episode-9.mp4\n",
      "Episode: 10 Reward: 9.0 loss: 0.003384070122228496 last rewards: 5.1\n",
      "Episode: 11 Reward: 8.0 loss: 0.005966637090273176 last rewards: 5.363636363636363\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(memory) \u001b[39m>\u001b[39m \u001b[39m1000\u001b[39m:\n\u001b[1;32m     59\u001b[0m     experiences_train \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39msample(batch_size)\n\u001b[0;32m---> 60\u001b[0m     episode_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain(experiences_train)\n\u001b[1;32m     61\u001b[0m     training_session \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     62\u001b[0m     steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[14], line 51\u001b[0m, in \u001b[0;36mDQN_agent.train\u001b[0;34m(self, experience_batch)\u001b[0m\n\u001b[1;32m     49\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mgather(\u001b[39m1\u001b[39m,indicies\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice))\n\u001b[1;32m     50\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y, pred)\n\u001b[0;32m---> 51\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     52\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     53\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Repos/NCML_project/rlenv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/Repos/NCML_project/rlenv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#initialize environment\n",
    "env = gym.make(\"ALE/Pacman-v5\", render_mode=\"rgb_array\")\n",
    "env.seed(543)\n",
    "env = gym.wrappers.AtariPreprocessing(env, screen_size=84, grayscale_obs=False, frame_skip=1, noop_max=30)\n",
    "env = gym.wrappers.RecordVideo(env, './videos', episode_trigger = lambda x: (x+1) % 5 == 0)# PATH\n",
    "# env = NoopResetEnv(env, noop_max=30)\n",
    "replay_buffer = Memory(5000)\n",
    "torch.manual_seed(53407)\n",
    "actions = range(env.action_space.n)\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "#hyperparams\n",
    "max_steps = 9999\n",
    "training_freq = 1\n",
    "copying_freq = 10\n",
    "batch_size = 64\n",
    "\n",
    "#initialize agent\n",
    "agent = DQN_agent(lr=2e-4,gamma=0.99)\n",
    "\n",
    "training_session = 0\n",
    "max_episode = 5000\n",
    "\n",
    "#loops until max_time is reached\n",
    "memory = Memory(10000)\n",
    "\n",
    "total_steps = 0\n",
    "last_rewards = []\n",
    "\n",
    "episodes = []\n",
    "losses = []\n",
    "logger = {\"episodes\":[],\"losses\":[]}\n",
    "for episode  in range(1,max_episode):\n",
    "    #get first states\n",
    "    state = env.reset()\n",
    "    state = state[0]/255\n",
    "    state = torch.Tensor(state)\n",
    "    state = adjust_to_torch(state)\n",
    "    #loops until experience_capacity is reached\n",
    "    episode_reward = 0 \n",
    "    episode_loss = 0\n",
    "    steps = 0\n",
    "    for i in range(1, max_steps):\n",
    "        #predict q-values and choose action\n",
    "        with torch.no_grad():\n",
    "            pred = agent.predict(state.to(device))\n",
    "        action = agent.action(pred)\n",
    "        #get next states\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if i == max_steps - 1:\n",
    "            print(\"Max steps reached.\")\n",
    "        next_state = adjust_to_torch(torch.tensor(next_state)/255)\n",
    "        experience = [state, action, reward, next_state, terminated] \n",
    "        memory.add(experience)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        state = next_state      \n",
    "        if len(memory) > 1000:\n",
    "            experiences_train = memory.sample(batch_size)\n",
    "            episode_loss += agent.train(experiences_train)\n",
    "            training_session += 1\n",
    "            steps += 1\n",
    "        total_steps += 1\n",
    "        steps = i\n",
    "    #if episode % 100 == 0:\n",
    "    if episode % copying_freq == 0:\n",
    "        agent.copy()\n",
    "    if episode % 2 == 0:\n",
    "        logger[\"episodes\"].append(episodes)\n",
    "        logger[\"losses\"].append(losses)\n",
    "        episodes = []\n",
    "        losses = []\n",
    "    if episode % 2 == 0:\n",
    "        with open('./logs/logger_ddqn.json', 'w') as fp:# PATH\n",
    "            json.dump(logger, fp)\n",
    "    last_rewards.append(episode_reward)\n",
    "    episodes.append(episode_reward)\n",
    "    losses.append(episode_loss/steps)\n",
    "    if steps > 0:\n",
    "      print(f\"Episode: {episode} Reward: {episode_reward} loss: {episode_loss/steps} last rewards: {sum(last_rewards)/len(last_rewards)}\")\n",
    "      \n",
    "torch.save(agent.pred_NN.state_dict(), \"./pred.pt\")\n",
    "torch.save(agent.target_NN.state_dict(), \"./target.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7746d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
