{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a147f0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ale_py\n",
    "#import shimmy\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "474f722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()   \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features=64*7*7 , out_features=512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=512, out_features=5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x)\n",
    "        x = torch.flatten(conv_out, start_dim=1)\n",
    "        return self.fc(x)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73e8ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    \n",
    "    def __init__(self,size):\n",
    "        self.size = size\n",
    "        self.experiences = []\n",
    "    \n",
    "    def sample(self,batch_size):\n",
    "        return random.choices(self.experiences, k=batch_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.experiences.append(experience)\n",
    "        if len(self.experiences) > self.size:\n",
    "            self.experiences.pop(0)\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.experiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fc12362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_agent:\n",
    "\n",
    "\n",
    "    def __init__(self, lr=0.0001 ,gamma=0.99, epsilon_params=(0.9,0.05,1000)):\n",
    "        # Get cpu, gpu or mps device for training.\n",
    "        self.device = (\n",
    "            \"cuda\"\n",
    "            if torch.cuda.is_available()\n",
    "            else \"mps\"\n",
    "            if torch.backends.mps.is_available()\n",
    "            else \"cpu\"\n",
    "        )\n",
    "        print(f\"Using {self.device} device\")\n",
    "        self.pred_NN = CNN().to(self.device)\n",
    "        self.target_NN = copy.deepcopy(self.pred_NN)\n",
    "        self.target_NN.eval()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon_start = epsilon_params[0]\n",
    "        self.epsilon_end = epsilon_params[1]\n",
    "        self.epsilon_decay = epsilon_params[2]\n",
    "        self.optimizer = torch.optim.AdamW(self.pred_NN.parameters(), lr=lr,amsgrad=True)\n",
    "        self.steps_done = 0\n",
    "        \n",
    "    def predict(self, x):\n",
    "        self.steps_done += 1\n",
    "        return self.pred_NN.forward(x)\n",
    "    \n",
    "    def action(self, pred):\n",
    "        eps = self.epsilon_end + (self.epsilon_start - self.epsilon_end) * math.exp(-1. * self.steps_done / self.epsilon_decay)\n",
    "        return (\n",
    "            random.randint(0, pred.size(dim=0) - 1)\n",
    "            if random.random() < eps\n",
    "            else torch.argmax(pred).item()\n",
    "        )\n",
    "    \n",
    "    def train(self, experience_batch):\n",
    "        loss_fn = nn.SmoothL1Loss()\n",
    "        epoch_loss = 0\n",
    "        states = torch.stack([experience_batch[i][0].squeeze(0) for i in range(len(experience_batch))])\n",
    "        actions = torch.tensor([experience_batch[i][1] for i in range(len(experience_batch))])\n",
    "        rewards = torch.tensor([experience_batch[i][2] for i in range(len(experience_batch))])#torch.tensor(experience_batch[:][2])\n",
    "        next_states = torch.stack([experience_batch[i][3].squeeze(0)  for i in range(len(experience_batch))])\n",
    "        terminated = torch.tensor([not experience_batch[i][4] for i in range(len(experience_batch))])\n",
    "        y = self.estimated_value( rewards, next_states, terminated)\n",
    "        self.optimizer.zero_grad()\n",
    "        pred = self.pred_NN(states)\n",
    "        indicies = torch.LongTensor(actions)\n",
    "        indicies =indicies.unsqueeze(dim=0).T\n",
    "        pred = pred.gather(1,indicies)\n",
    "        loss = loss_fn(y, pred)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        return epoch_loss\n",
    "        \n",
    "    def copy(self):\n",
    "        self.target_NN.load_state_dict(self.pred_NN.state_dict())  \n",
    "        \n",
    "    def estimated_value(self, reward, next_state, done):\n",
    "        with torch.no_grad():# vectorize it\n",
    "                target_pred = self.target_NN.forward(next_state)\n",
    "                max_pred = torch.max(target_pred,1)[0].unsqueeze(1)\n",
    "                done = done.unsqueeze(1)\n",
    "                target = reward.unsqueeze(1) + self.gamma * torch.mul(max_pred,done)\n",
    "        return target\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5357eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Enviroment with all usefull wrappers\n",
    "env = gym.make(\"ALE/Pacman-v5\")\n",
    "env.seed(543)\n",
    "env = gym.wrappers.AtariPreprocessing(env, screen_size=84, grayscale_obs=False, frame_skip=1, noop_max=30)\n",
    "# env = NoopResetEnv(env, noop_max=30)\n",
    "# env = EpisodicLifeEnv(env)\n",
    "# env = PyTorchFrame(env)\n",
    "# env = ClipRewardEnv(env)\n",
    "# env = WarpFrame(env)\n",
    "# env = FrameStack(env, 4)\n",
    "# env = gym.wrappers.Monitor(\n",
    "#           env, './video/', video_callable=lambda episode_id: episode_id % 100 == 0, force=True)\n",
    "replay_buffer = Memory(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a37195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_to_torch(t):\n",
    "    t = t.unsqueeze(dim=0)\n",
    "    return torch.movedim(t, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9751d961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Episode: 1 Reward: 2.0 loss: 0.005789495567906324 steps: 371\n",
      "Episode: 2 Reward: 2.0 loss: 0.002877372517648921 steps: 367\n",
      "Episode: 3 Reward: 6.0 loss: 0.002397717270509477 steps: 382\n",
      "Episode: 4 Reward: 6.0 loss: 0.003760337046779747 steps: 443\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(memory) \u001b[39m>\u001b[39m batch_size:\n\u001b[1;32m     47\u001b[0m         experiences_train \u001b[39m=\u001b[39m memory\u001b[39m.\u001b[39msample(batch_size)\n\u001b[0;32m---> 48\u001b[0m         episode_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mtrain(experiences_train)\n\u001b[1;32m     49\u001b[0m         training_session \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[39m#if episode % 100 == 0:\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[29], line 44\u001b[0m, in \u001b[0;36mDQN_agent.train\u001b[0;34m(self, experience_batch)\u001b[0m\n\u001b[1;32m     42\u001b[0m next_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([experience_batch[i][\u001b[39m3\u001b[39m]\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)  \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(experience_batch))])\n\u001b[1;32m     43\u001b[0m terminated \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mnot\u001b[39;00m experience_batch[i][\u001b[39m4\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(experience_batch))])\n\u001b[0;32m---> 44\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimated_value( rewards, next_states, terminated)\n\u001b[1;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     46\u001b[0m pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpred_NN(states)\n",
      "Cell \u001b[0;32mIn[29], line 61\u001b[0m, in \u001b[0;36mDQN_agent.estimated_value\u001b[0;34m(self, reward, next_state, done)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mestimated_value\u001b[39m(\u001b[39mself\u001b[39m, reward, next_state, done):\n\u001b[1;32m     60\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\u001b[39m# vectorize it\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m             target_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_NN\u001b[39m.\u001b[39;49mforward(next_state)\n\u001b[1;32m     62\u001b[0m             max_pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(target_pred,\u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m             done \u001b[39m=\u001b[39m done\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[27], line 21\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 21\u001b[0m     conv_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[1;32m     22\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(conv_out, start_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(x)\n",
      "File \u001b[0;32m~/Repos/NCML_project/rlenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Repos/NCML_project/rlenv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Repos/NCML_project/rlenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Repos/NCML_project/rlenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/Repos/NCML_project/rlenv/lib/python3.10/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#initialize environment\n",
    "actions = range(env.action_space.n)\n",
    "\n",
    "#hyperparams\n",
    "max_steps = 9999\n",
    "training_freq = 1\n",
    "copying_freq = 1000\n",
    "batch_size = 32\n",
    "\n",
    "#initialize agent\n",
    "agent = DQN_agent(lr=1e-4,gamma=0.99)\n",
    "\n",
    "training_session = 0\n",
    "max_episode = 5000\n",
    "\n",
    "#loops until max_time is reached\n",
    "memory = Memory(10000)\n",
    "\n",
    "total_steps = 0\n",
    "for episode  in range(1,max_episode):\n",
    "    #get first states\n",
    "    state = env.reset()\n",
    "    state = state[0]/255\n",
    "    state = torch.Tensor(state)\n",
    "    state = adjust_to_torch(state)\n",
    "    #loops until experience_capacity is reached\n",
    "    episode_reward = 0 \n",
    "    episode_loss = 0\n",
    "    steps = 0\n",
    "    for i in range(1, max_steps):\n",
    "        #predict q-values and choose action\n",
    "        with torch.no_grad():\n",
    "            pred = agent.predict(state)\n",
    "        action = agent.action(pred)\n",
    "        #get next states\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if i == max_steps - 1:\n",
    "            print(\"Max steps reached.\")\n",
    "        next_state = adjust_to_torch(torch.tensor(next_state)/255)\n",
    "        experience = [state, action, reward, next_state, terminated] \n",
    "        memory.add(experience)\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "        state = next_state      \n",
    "        if len(memory) > batch_size:\n",
    "            experiences_train = memory.sample(batch_size)\n",
    "            episode_loss += agent.train(experiences_train)\n",
    "            training_session += 1\n",
    "    #if episode % 100 == 0:\n",
    "        steps = i\n",
    "    if episode % copying_freq == 0:\n",
    "        agent.copy()\n",
    "    print(f\"Episode: {episode} Reward: {episode_reward} loss: {episode_loss/steps} steps: {steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8f810f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'*****stopped training after {elapsed_time} seconds*****\\n')\n",
    "\n",
    "print('*****Plot filters in first layer*****\\n')\n",
    "kernels = agent.pred_NN.stack[0].weight.detach().clone()\n",
    "kernels = kernels - kernels.min()\n",
    "kernels = kernels / kernels.max()\n",
    "filter_img = torchvision.utils.make_grid(kernels, nrow = 8)\n",
    "plt.imshow(filter_img.permute(1, 2, 0))\n",
    "plt.show() \n",
    "\n",
    "print('*****Pickle Dumping model*****\\n')\n",
    "\n",
    "file = open('safetyPickleDump', 'wb')\n",
    "pickle.dump(agent, file)\n",
    "file.close()\n",
    "\n",
    "print(\"*****Saving model*****\")\n",
    "torch.save(agent.pred_NN.state_dict(), \"model\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "53a28df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [[1,2,3,4],[1,2,3,4],[1,2,3,4]]\n",
    "A[0][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6d71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "543cfb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[1,2],[3,4],[5,6],[7,8]])\n",
    "indicies = torch.LongTensor([0,1,0,1])\n",
    "indicies =indicies.unsqueeze(dim=0).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fd9b34bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea04e3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [4],\n",
       "        [5],\n",
       "        [8]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.gather(1,indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7426a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
