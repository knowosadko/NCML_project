{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ale_py\n",
    "# if using gymnasium\n",
    "import shimmy\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "from torchviz import make_dot, make_dot_from_trace"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initilize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork,self).__init__()\n",
    "        self.common = nn.Linear(4, 128)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.critic = nn.Linear(128,1)\n",
    "        self.actor = nn.Linear(128,2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x)\n",
    "        x = torch.unsqueeze(x,dim=0)\n",
    "        x = self.common(x)\n",
    "        x = self.activation(x)\n",
    "        return self.softmax(self.actor(x)), self.critic(x)\n",
    "    \n",
    "    def calculate_loss(self, G, V, A ):\n",
    "        advantage = G - V\n",
    "        vector = -torch.mul(A,advantage)\n",
    "        loss_actor = vector.sum()\n",
    "        loss_critic = F.smooth_l1_loss(V, G, reduction='sum')\n",
    "        return loss_actor, loss_critic\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    \"\"\"\n",
    "    A class representing an Actor-Critic model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, gamma):\n",
    "        \"\"\"\n",
    "        Initializes the ActorCritic object.\n",
    "\n",
    "        Args:\n",
    "            gamma (float): The discount factor for calculating expected returns.\n",
    "        \"\"\"\n",
    "        self.nn_model = NeuralNetwork()\n",
    "        self.optimizer = torch.optim.Adam(self.nn_model.parameters(), lr=0.005)\n",
    "        self.gamma = gamma\n",
    "        self.values = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current state.\n",
    "\n",
    "        Args:\n",
    "            state: The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the selected action, its log probability, and the predicted value for the state.\n",
    "        \"\"\"\n",
    "        action_probs, value = self.nn_model(state)\n",
    "        m = Categorical(action_probs)\n",
    "        action = m.sample()\n",
    "        action_prob = m.log_prob(action)\n",
    "        return action.item(), action_prob, value.squeeze(dim=1)\n",
    "\n",
    "    def train_step(self):\n",
    "        \"\"\"\n",
    "        Performs a single training step on the neural network.\n",
    "        \"\"\"\n",
    "        V = torch.stack(self.values)\n",
    "        A = torch.stack(self.actions)\n",
    "        G = self.expected_return()\n",
    "        self.optimizer.zero_grad()\n",
    "        actor_loss, critic_loss = self.nn_model.calculate_loss(G, V, A)\n",
    "        loss = actor_loss + critic_loss\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def save_data(self, action_prob, value, reward):\n",
    "        \"\"\"\n",
    "        Saves the data from a single step in the training process.\n",
    "\n",
    "        Args:\n",
    "            action_prob: The log probability of the selected action.\n",
    "            value: The predicted value for the state.\n",
    "            reward: The reward received for the action.\n",
    "        \"\"\"\n",
    "        self.actions.append(action_prob)\n",
    "        self.values.append(value)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the values, actions, and rewards lists.\n",
    "        \"\"\"\n",
    "        self.values = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def expected_return(self):\n",
    "        \"\"\"\n",
    "        Calculates the expected return (discounted rewards) for the current episode.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The expected return values, normalized by subtracting the mean and dividing by the standard deviation.\n",
    "        \"\"\"\n",
    "        dis_reward = 0\n",
    "        g_array = []\n",
    "        for reward in reversed(self.rewards):\n",
    "            dis_reward = reward + self.gamma * dis_reward\n",
    "            g_array.insert(0, dis_reward)\n",
    "        g = torch.tensor(g_array).unsqueeze(dim=1)\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        g = (g - g.mean()) / (g.std()+eps)\n",
    "        return g "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 ended. Episode reward: 13.0 \n",
      "Episode 1 ended. Episode reward: 22.0 \n",
      "Episode 2 ended. Episode reward: 25.0 \n",
      "Episode 3 ended. Episode reward: 46.0 \n",
      "Episode 4 ended. Episode reward: 16.0 \n",
      "Episode 5 ended. Episode reward: 12.0 \n",
      "Episode 6 ended. Episode reward: 14.0 \n",
      "Episode 7 ended. Episode reward: 27.0 \n",
      "Episode 8 ended. Episode reward: 18.0 \n",
      "Episode 9 ended. Episode reward: 16.0 \n",
      "Episode 10 ended. Episode reward: 41.0 \n",
      "Episode 11 ended. Episode reward: 21.0 \n",
      "Episode 12 ended. Episode reward: 41.0 \n",
      "Episode 13 ended. Episode reward: 12.0 \n",
      "Episode 14 ended. Episode reward: 16.0 \n",
      "Episode 15 ended. Episode reward: 16.0 \n",
      "Episode 16 ended. Episode reward: 77.0 \n",
      "Episode 17 ended. Episode reward: 38.0 \n",
      "Episode 18 ended. Episode reward: 25.0 \n",
      "Episode 19 ended. Episode reward: 44.0 \n",
      "Episode 20 ended. Episode reward: 26.0 \n",
      "Episode 21 ended. Episode reward: 19.0 \n",
      "Episode 22 ended. Episode reward: 32.0 \n",
      "Episode 23 ended. Episode reward: 34.0 \n",
      "Episode 24 ended. Episode reward: 40.0 \n",
      "Episode 25 ended. Episode reward: 29.0 \n",
      "Episode 26 ended. Episode reward: 37.0 \n",
      "Episode 27 ended. Episode reward: 20.0 \n",
      "Episode 28 ended. Episode reward: 19.0 \n",
      "Episode 29 ended. Episode reward: 31.0 \n",
      "Episode 30 ended. Episode reward: 43.0 \n",
      "Episode 31 ended. Episode reward: 15.0 \n",
      "Episode 32 ended. Episode reward: 47.0 \n",
      "Episode 33 ended. Episode reward: 39.0 \n",
      "Episode 34 ended. Episode reward: 35.0 \n",
      "Episode 35 ended. Episode reward: 34.0 \n",
      "Episode 36 ended. Episode reward: 43.0 \n",
      "Episode 37 ended. Episode reward: 24.0 \n",
      "Episode 38 ended. Episode reward: 24.0 \n",
      "Episode 39 ended. Episode reward: 32.0 \n",
      "Episode 40 ended. Episode reward: 37.0 \n",
      "Episode 41 ended. Episode reward: 17.0 \n",
      "Episode 42 ended. Episode reward: 15.0 \n",
      "Episode 43 ended. Episode reward: 16.0 \n",
      "Episode 44 ended. Episode reward: 36.0 \n",
      "Episode 45 ended. Episode reward: 25.0 \n",
      "Episode 46 ended. Episode reward: 19.0 \n",
      "Episode 47 ended. Episode reward: 21.0 \n",
      "Episode 48 ended. Episode reward: 17.0 \n",
      "Episode 49 ended. Episode reward: 24.0 \n",
      "Episode 50 ended. Episode reward: 10.0 \n",
      "Episode 51 ended. Episode reward: 24.0 \n",
      "Episode 52 ended. Episode reward: 18.0 \n",
      "Episode 53 ended. Episode reward: 39.0 \n",
      "Episode 54 ended. Episode reward: 15.0 \n",
      "Episode 55 ended. Episode reward: 20.0 \n",
      "Episode 56 ended. Episode reward: 27.0 \n",
      "Episode 57 ended. Episode reward: 22.0 \n",
      "Episode 58 ended. Episode reward: 51.0 \n",
      "Episode 59 ended. Episode reward: 20.0 \n",
      "Episode 60 ended. Episode reward: 23.0 \n",
      "Episode 61 ended. Episode reward: 26.0 \n",
      "Episode 62 ended. Episode reward: 22.0 \n",
      "Episode 63 ended. Episode reward: 18.0 \n",
      "Episode 64 ended. Episode reward: 23.0 \n",
      "Episode 65 ended. Episode reward: 54.0 \n",
      "Episode 66 ended. Episode reward: 41.0 \n",
      "Episode 67 ended. Episode reward: 23.0 \n",
      "Episode 68 ended. Episode reward: 34.0 \n",
      "Episode 69 ended. Episode reward: 23.0 \n",
      "Episode 70 ended. Episode reward: 65.0 \n",
      "Episode 71 ended. Episode reward: 77.0 \n",
      "Episode 72 ended. Episode reward: 36.0 \n",
      "Episode 73 ended. Episode reward: 23.0 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m episode_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_steps):\n\u001b[0;32m---> 10\u001b[0m     action, action_prob, value \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mact(state)\n\u001b[1;32m     11\u001b[0m     state, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action) \n\u001b[1;32m     12\u001b[0m     model\u001b[39m.\u001b[39msave_data(action_prob,value,reward)\n",
      "Cell \u001b[0;32mIn[13], line 31\u001b[0m, in \u001b[0;36mActorCritic.act\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mSelects an action based on the current state.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39m    tuple: A tuple containing the selected action, its log probability, and the predicted value for the state.\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m action_probs, value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnn_model(state)\n\u001b[0;32m---> 31\u001b[0m m \u001b[39m=\u001b[39m Categorical(action_probs)\n\u001b[1;32m     32\u001b[0m action \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39msample()\n\u001b[1;32m     33\u001b[0m action_prob \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39mlog_prob(action)\n",
      "File \u001b[0;32m~/Documents/pyenv/lib/python3.10/site-packages/torch/distributions/categorical.py:66\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_events \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     65\u001b[0m batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39mndimension() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mSize()\n\u001b[0;32m---> 66\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/Documents/pyenv/lib/python3.10/site-packages/torch/distributions/distribution.py:60\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[1;32m     59\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, param)\n\u001b[0;32m---> 60\u001b[0m valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39;49mcheck(value)\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[1;32m     62\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/pyenv/lib/python3.10/site-packages/torch/distributions/constraints.py:406\u001b[0m, in \u001b[0;36m_Simplex.check\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[0;32m--> 406\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mall(value \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m&\u001b[39m ((value\u001b[39m.\u001b[39;49msum(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mabs() \u001b[39m<\u001b[39m \u001b[39m1e-6\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "torch.manual_seed(543)\n",
    "max_episodes = 10000\n",
    "max_steps = 9999\n",
    "model = ActorCritic(gamma=0.99)\n",
    "for episode in range(max_episodes): \n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    for step in range(1, max_steps):\n",
    "        action, action_prob, value = model.act(state)\n",
    "        state, reward, done, _, _ = env.step(action) \n",
    "        model.save_data(action_prob,value,reward)\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            observation, info = env.reset()\n",
    "            print(f\"Episode {episode} ended. Episode reward: {episode_reward} \")\n",
    "            break\n",
    "        if step == max_steps - 1:\n",
    "            print(\"Max steps reached.\")\n",
    "    model.train_step()\n",
    "    model.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
